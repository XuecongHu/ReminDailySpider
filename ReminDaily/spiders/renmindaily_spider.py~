#coding=utf-8
from scrapy.spider import Spider
from scrapy.selector import Selector
from sinaweibo.items import RmdailyItem
from scrapy import FormRequest
from scrapy import Request
import re,os,time
class ReMinDailySpider(Spider):
    name = 'rmdaily_spider'
    allowed_domains = ['rmrbw.info']
    start_urls=["http://rmrbw.info/"]  
    _main_site = "http://rmrbw.info/"
    
    def parse(self,response):
        sel = Selector(response)
        sites = sel.xpath("//div[@style='margin-left:40px']/table")
        #urls = []
        for site in sites:
            selector = site.xpath("./tr/td/h2/a")
            date = selector.xpath("text()").extract()[0]
            match = re.search("(\d+).*?",date,re.S)
            if match:
                year = int(match.group(1))
                if year>=1949 and year<= 1949:
                    url = selector.xpath("@href").extract()[0]
                    news_num = site.xpath("./tr/td[@style='padding-left:13px']/span/text()").extract()[0]
                    newsnum = int(news_num)
                    pages = newsnum/15 + 1
                    for page in range(1,pages+1):                  
                        #urls.append(self._main_site+url+"&page="+str(page))
                        nurl = self._main_site+url+"&page="+str(page)
                        yield Request(nurl,callback=self.parse_news_page)
       
        #for url in urls:
            #yield Request(url,callback=self.parse_news_page)
            
    def parse_news_page(self,response):
        sel = Selector(response)
        links = sel.xpath("//tr[@class='tr3 t_one']")
        #items = []
        stime = time.time()
        for link in links:
            item = RmdailyItem()
            url = link.xpath("./td[@style='text-align:left;padding-left:8px']/h3/a/@href").extract()[0]
            item['url'] = self._main_site+url
            title = link.xpath("./td[@style='text-align:left;padding-left:8px']/h3/a/text()").extract()[0]
            item['title'] = title
            asel = link.xpath("./td[@class='tal y-style']/a[@class='bl']/text()").extract()
            if asel:
                author = asel[0]
            else:
                author = 'null'
            item['author'] = author
            date = link.xpath("./td[@class='tal y-style']/div/text()").extract()[0]
            item['date'] = date
            yield Request(item['url'], callback=lambda response, item = item: self.parse_news(response,item))
            #items.append(item)
        etime = time.time()
        print "parse_news_page:"+str(etime-stime)
        #for ritem in items:
            #yield Request(ritem['url'], callback=lambda response, item=ritem: self.parse_news(response,item))
            
    def parse_news(self,response,item):
        stime = time.time()
        sel = Selector(response)
        contents = sel.xpath("//div[@class='tpc_content']/text()").extract()
        content = ""
        for con in contents:
            content+=con
        item['content'] = content
        print "获取新闻内容"
        self.write_to_file(item)
        etime = time.time()
        print "parse_news:"+str(etime-stime)
    
    def write_to_file(self,dict): 
        os.chdir('/home/frank/技术学习/web_spider/ReminDaily/rm_result')
        date = dict['date'].split('-')
        year = date[0]
        month = date[1]
        
        if os.path.exists(year) == False:
            os.mkdir(year)
        os.chdir(year)
        if os.path.exists(month) == False:
            os.mkdir(month)
        os.chdir(month)
        title = dict['title']
        with open(title,'w+') as f:
            date = dict['date']
            author = dict['author']
            url = dict['url']
            file_content = u"标题: "+title+"\n"+u"时间: "+date+u"\t作者: "+author+"\tURL:"+url+"\n"+dict['content']
            f.write(file_content.encode('gbk'))    
            print "成功写入文件"
        
            
        
    
